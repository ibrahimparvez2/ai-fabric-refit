
# üßµ Fashion Design Generator with Gemini + Stable Diffusion + ControlNet

This notebook lets you go from uploaded fabric images ‚Üí Gemini design prompt ‚Üí Stable Diffusion image generation.

---

## üì¶ Step 1: Install Dependencies

```python
!pip install -q google-generativeai
!pip install -q diffusers transformers accelerate safetensors
!pip install -q opencv-python
!pip install -q xformers --no-cache-dir
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```


---

## üîê Step 2: Authenticate with Google + Set API Key

```python
import os
from google.colab import userdata

# Gemini
GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')  # Add your key via colab secrets
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

# GCP (optional if using vision tagging from Colab)
!gcloud auth login --quiet
```


---

## üìÅ Step 3: Upload Fabric Images

```python
from google.colab import files
from PIL import Image
import io

uploaded = files.upload()

# Save and preview
fabric_images = []
for filename, data in uploaded.items():
    image = Image.open(io.BytesIO(data)).convert("RGB")
    image.save(filename)
    fabric_images.append(filename)

fabric_images
```


---

## üß† Step 4: Tag Fabrics via Gemini Vision

```python
import google.generativeai as genai
import base64
import json

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-pro-latest')

def analyze_image(file_path):
    with open(file_path, "rb") as f:
        image_data = f.read()
    prompt = [
        "Analyze this fabric image and provide detailed information about: "
        "material composition, color palette (including secondary colors), "
        "pattern characteristics, texture properties, and structural elements. "
        "Format as JSON with separate sections for each characteristic.",
        {"mime_type": "image/jpeg", "data": image_data}
    ]
    response = model.generate_content(prompt)
    response_text = response.text.strip().replace('```json\n', '').rstrip('```')
    return json.loads(response_text)

fabric_tags = {img: analyze_image(img) for img in fabric_images}
fabric_tags
```


---

## ‚úçÔ∏è Step 5: Generate Practical Design Prompt

```python
def generate_design_prompt(fabric_tags):
    descs = []
    for fname, tag in fabric_tags.items():
        desc = f"{fname}: {tag['color_palette']['primary_color']} {tag['material_composition']['likely_material']}, "
        desc += f"{tag['pattern_characteristics']['pattern_type']} pattern, {tag['texture_properties']['visual_texture']} texture"
        descs.append(desc)
    
    prompt = (
        "Design a practical, wearable garment that combines these materials in a simple, functional way. "
        "Focus on everyday wearability and comfort. DO NOT write a full exploration of themes. "
        "Keep the concept under 300 words.

Materials:
" +
        "\n".join(descs) +
        "\n\nDesign a simple, practical garment that combines these elements in a functional way."
    )
    return prompt

design_prompt = generate_design_prompt(fabric_tags)
print(design_prompt)
```


---

## üé® Step 6: Generate Design Image (ControlNet + Stable Diffusion)

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch
from PIL import Image
import numpy as np
import cv2

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
).to("cuda")
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_xformers_memory_efficient_attention()

# Use edge of first fabric as guide
image = Image.open(fabric_images[0]).resize((512, 512)).convert("RGB")
np_image = np.array(image)
edges = cv2.Canny(np_image, 100, 200)
control_image = Image.fromarray(edges).convert("RGB")

prompt = design_prompt

result = pipe(prompt=prompt, image=control_image, num_inference_steps=30, guidance_scale=7.5).images[0]
result.save("final_design.jpg")
result.show()
```


---

‚úÖ Now you‚Äôve got an image based on your uploaded textures and generated design prompt.

To integrate with Flask:
- Reuse `generate_design_prompt()` in your app
- Save uploaded textures ‚Üí pass to this notebook backend
- Send output `final_design.jpg` path to your frontend via `render_template`
